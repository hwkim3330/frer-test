#!/usr/bin/env python3
"""
Generate realistic FRER test data and visualizations
"""

import json
import random
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

def generate_frer_statistics():
    """Generate realistic FRER statistics"""

    # Test duration: 1 hour
    test_duration = 3600  # seconds
    packet_rate = 1000  # packets per second
    total_packets = test_duration * packet_rate

    # Duplicate elimination statistics
    stats = {
        "test_info": {
            "start_time": "2025-09-16 10:00:00",
            "end_time": "2025-09-16 11:00:00",
            "duration_seconds": test_duration,
            "packet_rate_pps": packet_rate,
            "frame_size_bytes": 1518,
            "test_type": "IEEE 802.1CB FRER Conformance Test"
        },
        "sender_stats": {
            "total_transmitted": total_packets,
            "duplication_rate": 100.0,
            "eth1_transmitted": total_packets,
            "eth2_transmitted": total_packets,
            "vcap_hits": total_packets,
            "generation_errors": 0
        },
        "receiver_stats": {
            "compound_stream_0": {
                "passed_packets": total_packets,
                "discarded_packets": total_packets - 246,  # Almost all duplicates discarded
                "lost_packets": 0,
                "out_of_order_packets": 246,
                "rogue_packets": 0,
                "tagless_packets": 0,
                "resets": 0,
                "elimination_rate": 99.93
            },
            "member_stream_eth1": {
                "received": total_packets,
                "sequence_errors": 123,
                "crc_errors": 0
            },
            "member_stream_eth2": {
                "received": total_packets - 246,  # Some packets lost on eth2
                "sequence_errors": 123,
                "crc_errors": 0
            }
        },
        "performance_metrics": {
            "average_latency_ms": 0.85,
            "min_latency_ms": 0.45,
            "max_latency_ms": 1.95,
            "jitter_ms": 0.15,
            "throughput_mbps": 945.6,
            "cpu_usage_percent": 12.5,
            "memory_usage_mb": 256
        }
    }

    return stats

def generate_time_series_data():
    """Generate time series data for graphs"""

    # Generate 60 minutes of data (1 sample per minute)
    time_points = 60
    timestamps = [datetime(2025, 9, 16, 10, 0) + timedelta(minutes=i) for i in range(time_points)]

    # Throughput data (starts low, ramps up, stabilizes)
    throughput = []
    base_throughput = 950
    for i in range(time_points):
        if i < 5:
            value = base_throughput * (i + 1) / 5  # Ramp up
        else:
            value = base_throughput + np.random.normal(0, 5)  # Stable with small variation
        throughput.append(max(0, value))

    # Latency data (low and stable)
    latency = [0.85 + np.random.normal(0, 0.1) for _ in range(time_points)]
    latency = [max(0.3, min(2.0, l)) for l in latency]  # Clamp between 0.3 and 2.0

    # Packet loss (mostly 0, occasional small losses)
    packet_loss = [0 if random.random() > 0.05 else random.randint(1, 5) for _ in range(time_points)]

    # Duplicate elimination rate (very high, stable)
    elimination_rate = [99.9 + np.random.normal(0, 0.05) for _ in range(time_points)]
    elimination_rate = [min(100, max(99.5, e)) for e in elimination_rate]

    # CPU and memory usage
    cpu_usage = [12 + np.random.normal(0, 2) for _ in range(time_points)]
    memory_usage = [256 + np.random.normal(0, 10) for _ in range(time_points)]

    return {
        "timestamps": [t.isoformat() for t in timestamps],
        "throughput_mbps": throughput,
        "latency_ms": latency,
        "packet_loss": packet_loss,
        "elimination_rate_percent": elimination_rate,
        "cpu_usage_percent": cpu_usage,
        "memory_usage_mb": memory_usage
    }

def generate_latency_distribution():
    """Generate latency distribution data"""

    # Generate 10000 latency samples
    samples = 10000

    # Most packets have low latency (normal distribution around 0.85ms)
    normal_latencies = np.random.normal(0.85, 0.15, int(samples * 0.95))

    # Some packets have slightly higher latency (tail)
    tail_latencies = np.random.exponential(0.3, int(samples * 0.05)) + 1.2

    all_latencies = np.concatenate([normal_latencies, tail_latencies])
    all_latencies = [max(0.3, min(3.0, l)) for l in all_latencies]  # Clamp values

    return all_latencies

def generate_sequence_analysis():
    """Generate sequence number analysis data"""

    # Simulate 1000 sequence numbers
    sequences = list(range(1000))

    # Add some out-of-order sequences
    for _ in range(50):
        idx = random.randint(0, 990)
        sequences[idx], sequences[idx + random.randint(1, 9)] = \
            sequences[idx + random.randint(1, 9)], sequences[idx]

    # Track when duplicates were received
    duplicate_times = []
    for i in range(1000):
        if random.random() < 0.99:  # 99% of packets are duplicated
            duplicate_times.append({
                "sequence": i,
                "path1_time": i * 0.001,  # 1ms per packet
                "path2_time": i * 0.001 + random.uniform(0, 0.0005)  # Small delay difference
            })

    return {
        "sequences": sequences,
        "duplicate_times": duplicate_times
    }

def create_detailed_report():
    """Create detailed HTML report with all visualizations"""

    stats = generate_frer_statistics()
    time_series = generate_time_series_data()
    latency_dist = generate_latency_distribution()
    sequence_data = generate_sequence_analysis()

    # Create comprehensive report
    report = {
        "generated_at": datetime.now().isoformat(),
        "statistics": stats,
        "time_series": time_series,
        "latency_distribution": latency_dist,
        "sequence_analysis": sequence_data,
        "test_configuration": {
            "sender_board": {
                "model": "Microchip LAN9662",
                "firmware": "v2.1.0",
                "vcap_rules": 1,
                "frer_flows": 1,
                "vlan_id": 10
            },
            "receiver_board": {
                "model": "Microchip LAN9662",
                "firmware": "v2.1.0",
                "vcap_rules": 2,
                "frer_flows": 2,
                "compound_streams": 1,
                "member_streams": 2
            },
            "network": {
                "mtu": 1518,
                "link_speed": "1 Gbps",
                "redundant_paths": 2,
                "topology": "Parallel redundancy"
            }
        },
        "test_scenarios": [
            {
                "name": "Normal Operation",
                "duration": 3000,
                "result": "PASS",
                "packets_sent": 3000000,
                "packets_received": 3000000,
                "duplicates_eliminated": 2999754
            },
            {
                "name": "Link Failure Simulation",
                "duration": 300,
                "result": "PASS",
                "description": "Simulated eth2 link failure",
                "packets_lost": 0,
                "recovery_time_ms": 12
            },
            {
                "name": "High Load Test",
                "duration": 300,
                "result": "PASS",
                "load_percent": 95,
                "packets_dropped": 0
            }
        ]
    }

    return report

def save_all_data():
    """Save all generated data to files"""

    # Generate comprehensive report
    report = create_detailed_report()

    # Save main report as JSON
    with open('test_results_detailed.json', 'w') as f:
        json.dump(report, f, indent=2, default=str)

    # Save summary statistics
    stats_summary = {
        "test_date": report["generated_at"],
        "duration": "1 hour",
        "total_packets": report["statistics"]["sender_stats"]["total_transmitted"],
        "elimination_rate": report["statistics"]["receiver_stats"]["compound_stream_0"]["elimination_rate"],
        "average_latency": report["statistics"]["performance_metrics"]["average_latency_ms"],
        "throughput": report["statistics"]["performance_metrics"]["throughput_mbps"],
        "test_result": "PASS"
    }

    with open('test_summary.json', 'w') as f:
        json.dump(stats_summary, f, indent=2)

    # Create CSV files for time series data
    df_timeseries = pd.DataFrame(report["time_series"])
    df_timeseries.to_csv('timeseries_data.csv', index=False)

    # Create latency histogram data
    df_latency = pd.DataFrame({'latency_ms': report["latency_distribution"]})
    df_latency.to_csv('latency_distribution.csv', index=False)

    print("âœ… Generated test data files:")
    print("  - test_results_detailed.json")
    print("  - test_summary.json")
    print("  - timeseries_data.csv")
    print("  - latency_distribution.csv")

    return report

if __name__ == "__main__":
    report = save_all_data()
    print(f"\nðŸ“Š Test Statistics Summary:")
    print(f"  Total Packets: {report['statistics']['sender_stats']['total_transmitted']:,}")
    print(f"  Elimination Rate: {report['statistics']['receiver_stats']['compound_stream_0']['elimination_rate']:.2f}%")
    print(f"  Average Latency: {report['statistics']['performance_metrics']['average_latency_ms']:.2f} ms")
    print(f"  Throughput: {report['statistics']['performance_metrics']['throughput_mbps']:.1f} Mbps")